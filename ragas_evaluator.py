"""
Evaluate the smart shopping recommendation pipeline using the RAGAS framework.

This script is designed to be run in an environment where the `ragas` library
is installed along with its dependencies (e.g., `datasets`, `evaluate` and
`ragas.metrics`).  It automates the creation of a synthetic test dataset
and computes core RAGAS metrics (faithfulness, answer relevancy, context
precision and context recall) against the responses generated by our
LangGraph‑based recommendation system.

By default the script uses a small hand‑crafted test set aligned with
``mock_data.json``.  If you set the environment variable
``USE_RAGAS_TESTSET`` to a truthy value (e.g., ``1`` or ``true``), the
script will instead call Ragas’s built‑in testset generator to synthesize
questions directly from your Reddit documents.  You can control the
number of generated questions via ``RAGAS_TESTSET_SIZE``.  This allows
you to explore the pipeline on a range of automatically generated
queries without manually defining them.

The pipeline mirrors the behaviour of ``main.py``: it first uses the LLM
to map a user's goal to one or more shopping categories, then retrieves
the most relevant community comments for each category and finally
synthesises a friendly recommendation summary.  For evaluation
purposes, we extract the raw comments that were used to make the
recommendations and supply them as contexts to RAGAS.  When using the
hand‑crafted test set we also specify a simple ground truth answer for
each question (namely the product that the community most endorses) to
allow RAGAS to compute context recall and precision.  When using the
Ragas generator, the ground truth column is left empty because
automatically generated questions do not map cleanly to a single
product.

To run this evaluation locally:

    1. Ensure you have installed the ragas library, for example:

       pip install ragas

    2. Set your OpenAI API key in the environment (OPENAI_API_KEY) if
       semantic retrieval or LLM summarisation is required.
    3. Optionally choose a retrieval strategy via RETRIEVER_STRATEGY.  If
       unspecified, the script defaults to ``naive``.
    4. Execute this script.  It will print a DataFrame of metric scores
       computed over the synthetic test set.

Note: Because RAGAS makes calls to LLMs under the hood to evaluate
faithfulness and relevance, running this script will incur API usage.
"""

from __future__ import annotations

import os
import json
from typing import List, Tuple, Dict, Any, Optional
from dotenv import load_dotenv

try:
    # The ragas package provides the evaluation and metrics functionality.
    from ragas.evaluation import evaluate
    from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
except ImportError as exc:
    raise ImportError(
        "The 'ragas' package is required to run this script. "
        "Install it via 'pip install ragas' in your local environment."
    ) from exc

from datasets import Dataset

# Import the functions from main.py.  These functions implement the goal
# analysis, retrieval and synthesis logic of the smart shopping demo.
from main import (
    analyze_goal,
    simple_reddit_retriever,
    advanced_reddit_retriever,
    lexical_reddit_retriever,
    mmr_reddit_retriever,
    semantic_reddit_retriever,
    tfidf_reddit_retriever,
    synthesizer,
)


load_dotenv()

def _get_retriever(strategy: str):
    """Return the appropriate retrieval function based on a strategy name.

    Args:
        strategy: One of ``naive``, ``advanced``, ``lexical``, ``mmr``,
          ``semantic`` or ``tfidf``.  Retrieval names are case insensitive.

    Returns:
        A callable that accepts a ToolkitState and returns a ToolkitState.
    """
    strat = strategy.lower()
    if strat == "advanced":
        return advanced_reddit_retriever
    if strat == "lexical":
        return lexical_reddit_retriever
    if strat == "mmr":
        return mmr_reddit_retriever
    if strat in ("semantic", "embedding"):
        return semantic_reddit_retriever
    if strat in ("tfidf", "vectorizer"):
        return tfidf_reddit_retriever
    # Support additional strategies defined in main.py
    if strat in ("bm25",):
        from main import bm25_reddit_retriever  # type: ignore
        return bm25_reddit_retriever
    if strat in ("compression", "compress"):
        from main import compression_reddit_retriever  # type: ignore
        return compression_reddit_retriever
    if strat in ("multiquery", "multi-query", "multi"):
        from main import multiquery_reddit_retriever  # type: ignore
        return multiquery_reddit_retriever
    if strat in ("parent", "parentdocument", "parent-document", "parent_doc"):
        from main import parent_document_retriever  # type: ignore
        return parent_document_retriever
    if strat == "ensemble":
        from main import ensemble_reddit_retriever  # type: ignore
        return ensemble_reddit_retriever
    # Default to naïve
    return simple_reddit_retriever


def _run_pipeline(user_goal: str, strategy: str) -> Tuple[str, List[str]]:
    """Run the recommendation pipeline for a single goal and strategy.

    This helper reconstructs the state expected by the agentic workflow in
    main.py.  It performs three steps: goal analysis → retrieval →
    synthesis.  The returned answer is the final summary produced by the
    synthesiser and the contexts are extracted from the retrieved comments.

    Args:
        user_goal: The free‑form question or goal posed by the user.
        strategy: The retrieval strategy name (see `_get_retriever`).

    Returns:
        A tuple containing (answer, contexts), where answer is a
        human‑readable recommendation summary and contexts is a list of
        comment texts that were used to produce the answer.  These
        comments serve as the evidence for RAGAS evaluation.
    """
    # Build the initial state as expected by the graph functions
    state: Dict[str, any] = {
        "user_goal": user_goal,
        "category_plan": [],
        "recommendations": {},
        "final_output": "",
    }
    # Use the LLM to identify categories
    state = analyze_goal(state)
    # Select and apply the retrieval strategy
    retriever = _get_retriever(strategy)
    state = retriever(state)
    # Use the LLM to synthesise a final answer
    state = synthesizer(state)  # type: ignore  # synthesizer returns updated state
    answer = state.get("final_output", "")
    # Gather all comment texts from the recommendations for use as contexts
    contexts: List[str] = []
    for rec in state.get("recommendations", {}).values():
        if isinstance(rec, dict) and "top_comments" in rec:
            for c in rec.get("top_comments", []):
                comment = c.get("comment")
                if comment:
                    contexts.append(comment)
        elif isinstance(rec, dict):
            comment = rec.get("comment")
            if comment:
                contexts.append(comment)
    return answer, contexts


def _load_langchain_docs() -> List[Any]:
    """
    Convert the mock Reddit comments into LangChain ``Document`` objects.

    This helper reads the ``mock_data.json`` file used by the application,
    flattens the nested comment structure and wraps each comment in a
    ``langchain.schema.Document`` so that it can be passed directly to
    Ragas's testset generator.  Metadata is preserved to provide
    additional context if desired.

    Returns:
        A list of LangChain Document instances, one per comment.
    """
    try:
        from langchain.schema import Document
    except ImportError:
        raise ImportError(
            "langchain must be installed to generate a testset from documents."
        )
    # Locate the mock data file relative to this script
    data_file = os.getenv("DATA_FILE", "mock_data.json")
    with open(data_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    docs: List[Any] = []
    for post in data:
        category = post.get("category", "").strip()
        subreddit = post.get("subreddit", "").strip()
        for c in post.get("context", []):
            text = c.get("comment", "").strip()
            # Skip empty comments
            if not text:
                continue
            metadata = {
                "product": c.get("product"),
                "votes": c.get("votes"),
                "category": category,
                "subreddit": subreddit,
            }
            docs.append(Document(page_content=text, metadata=metadata))
    return docs


def build_dataset(
    strategy: str,
    use_ragas_generator: Optional[bool] = None,
    cache_path: Optional[str] = None,
) -> Dataset:
    """Construct a HuggingFace Dataset for evaluation.

    By default this function uses a small synthetic test set aligned with
    ``mock_data.json``.  When ``use_ragas_generator`` is True (or when the
    environment variable ``USE_RAGAS_TESTSET`` is set to a truthy value),
    it will invoke Ragas's built‑in testset generator to synthesize
    questions automatically from your documents.  The generated test
    cases are then run through the full recommendation pipeline to
    produce answers and contexts.

    Args:
        strategy: The retrieval strategy name passed to `_run_pipeline`.
        use_ragas_generator: If True, generate the testset using Ragas; if
            False, use the predefined synthetic questions.  When None,
            consult the ``USE_RAGAS_TESTSET`` environment variable.

    Returns:
        A HuggingFace Dataset with columns ``question``, ``answer``,
        ``contexts`` (list of strings) and ``ground_truth``.  When using
        the Ragas generator, the ``ground_truth`` column is empty since
        there is no single correct product associated with automatically
        generated questions.
    """
    # If a cache file is provided and exists, load it instead of regenerating.
    if cache_path is None:
        cache_path = os.getenv("TESTSET_CACHE_PATH")
    if cache_path:
        cache_path = cache_path.strip()
        if os.path.exists(cache_path):
            # Load the cached dataset from JSON and return it
            try:
                import json as _json
                from datasets import Dataset as _Dataset
                with open(cache_path, "r", encoding="utf-8") as fp:
                    data = _json.load(fp)
                # Expecting keys: question, answer, contexts, ground_truth
                if isinstance(data, dict) and all(k in data for k in ["question", "answer", "contexts", "ground_truth"]):
                    return _Dataset.from_dict(data)
            except Exception:
                # If loading fails, fall back to regeneration
                pass

    # Resolve use_ragas_generator from argument or environment variable
    if use_ragas_generator is None:
        use_ragas_generator = os.getenv("USE_RAGAS_TESTSET", "").lower() in (
            "1", "true", "yes", "on"
        )

    # If asked to use the Ragas generator, attempt to load it
    if use_ragas_generator:
        # Import Ragas's testset generator lazily to avoid import errors
        try:
            from ragas.testset import TestsetGenerator
            from ragas.llms import LangchainLLMWrapper
            from ragas.embeddings import LangchainEmbeddingsWrapper
        except ImportError as exc:
            raise ImportError(
                "Ragas must be installed to use the automatic testset generator."
            ) from exc
        # Wrap the same LLM and embedding model used by the pipeline
        try:
            # Import the generator model used in main.py (GPT‑4o) via Langchain
            from langchain_openai import ChatOpenAI
            from langchain_openai import OpenAIEmbeddings as LCEmbeddings
        except Exception:
            raise ImportError(
                "langchain_openai is required for LLM wrappers."
            )
        # Use the same LLM as the synthesiser in main.py; adjust model name if needed
        llm = ChatOpenAI(model="gpt-4o")
        embedding_model = LCEmbeddings(model="text-embedding-3-small")
        generator_llm = LangchainLLMWrapper(llm)
        generator_embeddings = LangchainEmbeddingsWrapper(embedding_model)
        # Convert Reddit comments into LangChain documents
        docs = _load_langchain_docs()
        # Instantiate the testset generator
        gen = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)
        # Generate a small testset; the size can be configured via env var
        size_str = os.getenv("RAGAS_TESTSET_SIZE", "3")
        try:
            testset_size = int(size_str)
        except ValueError:
            testset_size = 3
        ragas_testset = gen.generate_with_langchain_docs(docs, testset_size=testset_size)
        # Build our evaluation dataset by running the full pipeline on each ragas‑generated query
        questions: List[str] = []
        answers: List[str] = []
        contexts_list: List[List[str]] = []
        ground_truths: List[str] = []
        for example in ragas_testset:
            query = example["query"]
            # Run our pipeline to get answer and contexts
            ans, ctxs = _run_pipeline(query, strategy)
            questions.append(query)
            answers.append(ans)
            contexts_list.append(ctxs)
            # There is no definitive ground truth for automatically generated questions
            ground_truths.append("")
        dataset = Dataset.from_dict(
            {
                "question": questions,
                "answer": answers,
                "contexts": contexts_list,
                "ground_truth": ground_truths,
            }
        )
        # If a cache path is provided, save the dataset to JSON for future use
        if cache_path:
            try:
                with open(cache_path, "w", encoding="utf-8") as fp:
                    json.dump(
                        {
                            "question": questions,
                            "answer": answers,
                            "contexts": contexts_list,
                            "ground_truth": ground_truths,
                        },
                        fp,
                        ensure_ascii=False,
                        indent=2,
                    )
            except Exception:
                pass
        return dataset

    # Otherwise fall back to the predefined synthetic test set matching our mock data
    tests = [
        {
            "query": "What's the best espresso machine under $500?",
            "expected": "Gaggia Classic Pro",
        },
        {
            "query": "Best budget noise cancelling headphones?",
            "expected": "Anker Soundcore Q30",
        },
        {
            "query": "What's a good lightweight hiking backpack for multi-day treks?",
            "expected": "Osprey Exos 48",
        },
    ]
    questions: List[str] = []
    answers: List[str] = []
    contexts_list: List[List[str]] = []
    ground_truths: List[str] = []
    for t in tests:
        q = t["query"]
        questions.append(q)
        # Run the pipeline to obtain answer and contexts
        ans, ctxs = _run_pipeline(q, strategy)
        answers.append(ans)
        contexts_list.append(ctxs)
        ground_truths.append(t["expected"])
    return Dataset.from_dict(
        {
            "question": questions,
            "answer": answers,
            "contexts": contexts_list,
            "ground_truth": ground_truths,
        }
    )


def main() -> None:
    """Entry point for the RAGAS evaluator.

    Reads the retrieval strategy from the environment, builds a test
    dataset, evaluates it with RAGAS and prints the resulting metrics
    as a pandas DataFrame.
    """
    strategy = os.getenv("RETRIEVER_STRATEGY", "naive")
    print(f"Using retrieval strategy: {strategy}")
    dataset = build_dataset(strategy)
    # Execute evaluation; note that RAGAS will contact the LLM provider
    # internally, so ensure your API key is available.
    result = evaluate(
        dataset,
        metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
    )
    # Convert to pandas for a nicer display
    df = result.to_pandas()
    print(df)


if __name__ == "__main__":
    main()
